{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "z2tmvmkSWAzG",
      "metadata": {
        "id": "z2tmvmkSWAzG"
      },
      "source": [
        "# Env\n",
        "Install some dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "SGvxUYlTT9zM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGvxUYlTT9zM",
        "outputId": "f53e665c-9c00-4037-8197-7d403d6cc0af"
      },
      "outputs": [],
      "source": [
        "# reinstall kaggle new version\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "JsZ9wC8YCxG9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsZ9wC8YCxG9",
        "outputId": "661c2591-d8a3-4db9-d9c2-752a704b7307"
      },
      "outputs": [],
      "source": [
        "# install ipdb for debugging\n",
        "! pip install ipdb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uti7zggJbreY",
      "metadata": {
        "id": "Uti7zggJbreY"
      },
      "source": [
        "# Mount Google Drive\n",
        "Mount google drive and set path for loading and saving both data and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Bm3FAmLCbt-M",
      "metadata": {
        "id": "Bm3FAmLCbt-M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cnHV9oeIb7k7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnHV9oeIb7k7",
        "outputId": "8f0ccb60-5a20-461b-a085-537860743ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "M0oTcaY4dPAj",
      "metadata": {
        "id": "M0oTcaY4dPAj"
      },
      "outputs": [],
      "source": [
        "# set project path\n",
        "project_path = 'dl-hw1'\n",
        "\n",
        "DEV_PATH = '/content/drive/MyDrive/CMU/dev'\n",
        "PRJ_PATH = os.path.join(DEV_PATH, project_path)\n",
        "INITDATA_PATH = os.path.join(DEV_PATH, project_path, 'data')\n",
        "MODEL_PATH = os.path.join(DEV_PATH, project_path, 'model')\n",
        "LOG_PATH = os.path.join(DEV_PATH, project_path, 'log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "FOoxuE8adYnA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOoxuE8adYnA",
        "outputId": "635f91b4-6e19-4fe0-f46f-d503fc6710ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create dir: /content/drive/MyDrive/CMU/dev/dl-hw1\n",
            "Create dir: /content/drive/MyDrive/CMU/dev/dl-hw1/data\n",
            "Create dir: /content/drive/MyDrive/CMU/dev/dl-hw1/model\n",
            "Create dir: /content/drive/MyDrive/CMU/dev/dl-hw1/log\n"
          ]
        }
      ],
      "source": [
        "# create project dir\n",
        "def mkdir(p):\n",
        "    if os.path.isdir(p) is False:\n",
        "        os.mkdir(p)\n",
        "        print(\"Create dir: {}\".format(p))\n",
        "\n",
        "mkdir(PRJ_PATH)\n",
        "mkdir(INITDATA_PATH)\n",
        "mkdir(MODEL_PATH)\n",
        "mkdir(LOG_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lgB3DeunTa3A",
      "metadata": {
        "id": "lgB3DeunTa3A"
      },
      "source": [
        "# Load data\n",
        "Load data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J5EeIZvYZGYI",
      "metadata": {
        "id": "J5EeIZvYZGYI"
      },
      "source": [
        "## Initial download\n",
        "from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i2asZsjKT_R9",
      "metadata": {
        "id": "i2asZsjKT_R9"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hat34rpfTJLH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hat34rpfTJLH",
        "outputId": "6a8514a8-99dc-4ec9-e1c4-aff0350273ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: kaggle config set [-h] -n NAME -v VALUE\n",
            "kaggle config set: error: argument -v/--value: expected one argument\n"
          ]
        }
      ],
      "source": [
        "# copy kaggle authentication\n",
        "kaggle_file = os.path.join(DEV_PATH, '.kaggle/kaggle.json')\n",
        "! mkdir /root/.kaggle\n",
        "! cp $kaggle_file /root/.kaggle/\n",
        "! chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# set default dir\n",
        "! kaggle config set -n path -v $INITDATA_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nIXcktMNTaXX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIXcktMNTaXX",
        "outputId": "cbee5b75-0cd7-4c11-a127-05b1de449ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-s22-hw1p2.zip to /content/drive/MyDrive/dev/data/dl-hw1/competitions/11-785-s22-hw1p2\n",
            "100% 1.85G/1.86G [00:23<00:00, 105MB/s]\n",
            "100% 1.86G/1.86G [00:23<00:00, 86.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# download competition data\n",
        "! kaggle competitions download -c 11-785-s22-hw1p2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eU2be0pcX9AW",
      "metadata": {
        "id": "eU2be0pcX9AW"
      },
      "source": [
        "## Load existing data\n",
        "from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "GTCqCx4nYEak",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTCqCx4nYEak",
        "outputId": "621fdcce-2bff-4d95-991e-5ae4cbfa314c"
      },
      "outputs": [],
      "source": [
        "# load data from drive\n",
        "zip_file = kaggle_file = os.path.join(INITDATA_PATH, 'competitions/11-785-s22-hw1p2/11-785-s22-hw1p2.zip')\n",
        "! unzip $zip_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HecRrAE6YHoW",
      "metadata": {
        "id": "HecRrAE6YHoW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3ggXQgug2CLS",
      "metadata": {
        "id": "3ggXQgug2CLS"
      },
      "source": [
        "# Tensorboard\n",
        "Setup Tensorboard to monitor performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ML8XkK7v20Kc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML8XkK7v20Kc",
        "outputId": "80373aab-a0e6-4031-8a87-3326de6a4d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will list all experiments that you've uploaded to\n",
            "https://tensorboard.dev. TensorBoard.dev experiments are visible\n",
            "to everyone. Do not upload sensitive data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) yes\n",
            "\n",
            "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=373649185512-8v619h5kft38l4456nm2dj4ubeqsrvh6.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&state=UdCJ5SDzdm1EV97xKa6OKps440bS63&prompt=consent&access_type=offline\n",
            "Enter the authorization code: 4/1AX4XfWjlPSm6rF0HdYyaGqHTHvxkUA2o2_71j1yvcstEsht8d8Tp16Lpw9o\n",
            "\n",
            "https://tensorboard.dev/experiment/9LIkaWzxSAuVD02kWJhb6g/\n",
            "\tName                 [No Name]\n",
            "\tDescription          hw1p2\n",
            "\tId                   9LIkaWzxSAuVD02kWJhb6g\n",
            "\tCreated              2022-02-06 04:02:54\n",
            "\tUpdated              2022-02-06 05:28:58\n",
            "\tRuns                 9\n",
            "\tTags                 3\n",
            "\tScalars              1038\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/O3Kvm9zPTMqzmKGGRYCbJQ/\n",
            "\tName                 [No Name]\n",
            "\tDescription          hw1p2\n",
            "\tId                   O3Kvm9zPTMqzmKGGRYCbJQ\n",
            "\tCreated              2022-02-04 18:52:41\n",
            "\tUpdated              2022-02-05 03:10:17\n",
            "\tRuns                 8\n",
            "\tTags                 3\n",
            "\tScalars              885\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/ZVdtZ63sQOCx5GRYvwFx4Q/\n",
            "\tName                 [No Name]\n",
            "\tDescription          hw1p2\n",
            "\tId                   ZVdtZ63sQOCx5GRYvwFx4Q\n",
            "\tCreated              2022-02-04 07:08:06\n",
            "\tUpdated              2022-02-04 15:20:06\n",
            "\tRuns                 6\n",
            "\tTags                 3\n",
            "\tScalars              801\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/dTPMXbDyTVWNGwTkiYj4WQ/\n",
            "\tName                 [No Name]\n",
            "\tDescription          hw1p2\n",
            "\tId                   dTPMXbDyTVWNGwTkiYj4WQ\n",
            "\tCreated              2022-02-03 22:34:22\n",
            "\tUpdated              2022-02-04 01:45:17\n",
            "\tRuns                 5\n",
            "\tTags                 3\n",
            "\tScalars              501\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/KpRIk96qSROBbvklBZBDyw/\n",
            "\tName                 [No Name]\n",
            "\tDescription          hw1p2\n",
            "\tId                   KpRIk96qSROBbvklBZBDyw\n",
            "\tCreated              2022-02-03 14:59:05\n",
            "\tUpdated              2022-02-03 21:09:54\n",
            "\tRuns                 4\n",
            "\tTags                 3\n",
            "\tScalars              351\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/c3DqVduvS8WY9DX9vEivHw/\n",
            "\tName                 [No Name]\n",
            "\tDescription          hw1p2\n",
            "\tId                   c3DqVduvS8WY9DX9vEivHw\n",
            "\tCreated              2022-02-03 05:26:52\n",
            "\tUpdated              2022-02-03 13:03:19\n",
            "\tRuns                 3\n",
            "\tTags                 3\n",
            "\tScalars              246\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/U4v1tGKjRWuvYtd3Hq48Fw/\n",
            "\tName                 [No Name]\n",
            "\tDescription          hw1p2\n",
            "\tId                   U4v1tGKjRWuvYtd3Hq48Fw\n",
            "\tCreated              2022-02-02 15:58:40\n",
            "\tUpdated              2022-02-03 00:09:48\n",
            "\tRuns                 1\n",
            "\tTags                 3\n",
            "\tScalars              150\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/nzfqotqkQdiRQafwOcvMQA/\n",
            "\tName                 [No Name]\n",
            "\tDescription          [No Description]\n",
            "\tId                   nzfqotqkQdiRQafwOcvMQA\n",
            "\tCreated              2022-02-02 07:03:57\n",
            "\tUpdated              2022-02-02 14:18:57\n",
            "\tRuns                 5\n",
            "\tTags                 3\n",
            "\tScalars              378\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/YgKUmV7GQiebPsaJqOfe7g/\n",
            "\tName                 context_02\n",
            "\tDescription          Adjust context\n",
            "\tId                   YgKUmV7GQiebPsaJqOfe7g\n",
            "\tCreated              2022-02-01 18:22:23\n",
            "\tUpdated              2022-02-02 06:54:20\n",
            "\tRuns                 3\n",
            "\tTags                 3\n",
            "\tScalars              270\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/wPMo9dzrRDG3cjphqQMxmA/\n",
            "\tName                 lr_01\n",
            "\tDescription          Adjust learning rate\n",
            "\tId                   wPMo9dzrRDG3cjphqQMxmA\n",
            "\tCreated              2022-02-01 15:20:04\n",
            "\tUpdated              2022-02-01 18:11:13\n",
            "\tRuns                 1\n",
            "\tTags                 3\n",
            "\tScalars              57\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/04raqejARniVgR0vZlUe7A/\n",
            "\tName                 context_02\n",
            "\tDescription          Adding context to the model\n",
            "\tId                   04raqejARniVgR0vZlUe7A\n",
            "\tCreated              2022-02-01 04:02:13\n",
            "\tUpdated              2022-02-01 08:46:38\n",
            "\tRuns                 1\n",
            "\tTags                 3\n",
            "\tScalars              96\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/57f0VODaRl6BYNrKd3gkpA/\n",
            "\tName                 base02\n",
            "\tDescription          Add 1 more layer\n",
            "\tId                   57f0VODaRl6BYNrKd3gkpA\n",
            "\tCreated              2022-01-31 06:50:51\n",
            "\tUpdated              2022-01-31 12:38:04\n",
            "\tRuns                 2\n",
            "\tTags                 3\n",
            "\tScalars              156\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/OcDgQe9IRIKBT8BJ6M2dFA/\n",
            "\tName                 test03\n",
            "\tDescription          [No Description]\n",
            "\tId                   OcDgQe9IRIKBT8BJ6M2dFA\n",
            "\tCreated              2022-01-30 20:28:11\n",
            "\tUpdated              2022-01-30 20:37:26\n",
            "\tRuns                 2\n",
            "\tTags                 3\n",
            "\tScalars              130\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/bcXgRuMaSXuah6bdyqK1vw/\n",
            "\tName                 test02\n",
            "\tDescription          02\n",
            "\tId                   bcXgRuMaSXuah6bdyqK1vw\n",
            "\tCreated              2022-01-30 20:18:36\n",
            "\tUpdated              2022-01-30 20:18:37\n",
            "\tRuns                 1\n",
            "\tTags                 3\n",
            "\tScalars              70\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "https://tensorboard.dev/experiment/mKr4MqfDS5mVRel3au7V3w/\n",
            "\tName                 test02\n",
            "\tDescription          02\n",
            "\tId                   mKr4MqfDS5mVRel3au7V3w\n",
            "\tCreated              2022-01-30 20:11:31\n",
            "\tUpdated              2022-01-30 20:14:11\n",
            "\tRuns                 1\n",
            "\tTags                 3\n",
            "\tScalars              70\n",
            "\tTensor bytes         0\n",
            "\tBinary object bytes  0\n",
            "Total: 15 experiment(s)\n"
          ]
        }
      ],
      "source": [
        "# setup tensorboard login\n",
        "! tensorboard dev list\n",
        "\n",
        "# upload during training (run in console)\n",
        "# tensorboard dev upload --logdir /content/drive/MyDrive/CMU/dev/dl-hw1/log --description \"hw1p2\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ANlV727juvP0",
      "metadata": {
        "id": "ANlV727juvP0"
      },
      "source": [
        "# Setup\n",
        "Setup script for \n",
        "- Data loading\n",
        "- Model training, validating, testing\n",
        "- Model saving, running, resuming, predicting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "PzGU1-ua8RCu",
      "metadata": {
        "id": "PzGU1-ua8RCu"
      },
      "outputs": [],
      "source": [
        "# load packages\n",
        "import os\n",
        "import datetime\n",
        "import pytz\n",
        "import time\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import tensorboard\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "OmF1eFfn87JU",
      "metadata": {
        "id": "OmF1eFfn87JU"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"dev-clean\", csvpath=None):\n",
        "        '''\n",
        "        Batch download files\n",
        "            data_path: data path\n",
        "            sample: how many npy files will be preloaded for one __getitem__ call\n",
        "            shuffle: shuffle or not\n",
        "            partition: directory to load file\n",
        "            csvpath: test by csvfile\n",
        "        '''\n",
        "        self.sample = sample \n",
        "        \n",
        "        # find x and y directory\n",
        "        self.X_dir = os.path.join(data_path, partition, 'mfcc')\n",
        "        self.Y_dir = os.path.join(data_path, partition, 'transcript')\n",
        "        \n",
        "        # find all files in the directory\n",
        "        self.X_names = os.listdir(self.X_dir)\n",
        "        self.Y_names = os.listdir(self.Y_dir)\n",
        "\n",
        "        # using a small part of the dataset to debug\n",
        "        if csvpath:\n",
        "            subset = self.parse_csv(csvpath)\n",
        "            self.X_names = [i for i in self.X_names if i in subset]\n",
        "            self.Y_names = [i for i in self.Y_names if i in subset]\n",
        "        \n",
        "        # random files in the path\n",
        "        if shuffle == True:\n",
        "            XY_names = list(zip(self.X_names, self.Y_names))\n",
        "            random.shuffle(XY_names)\n",
        "            self.X_names, self.Y_names = zip(*XY_names)\n",
        "        \n",
        "        # find amount of file\n",
        "        assert(len(self.X_names) == len(self.Y_names))\n",
        "        self.length = len(self.X_names)\n",
        "        \n",
        "        # set y label\n",
        "        self.PHONEMES = [\n",
        "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>'] # start of sentence / end of sentence\n",
        "    \n",
        "    # for loading csv\n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row[1])\n",
        "        return subset[1:]\n",
        "\n",
        "    # for getting the length\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.length / self.sample))\n",
        "    \n",
        "    # call when getting one file data\n",
        "    def __getitem__(self, i):\n",
        "        sample_range = range(i*self.sample, min((i+1)*self.sample, self.length))\n",
        "        \n",
        "        # load data for each np file\n",
        "        X, Y = [], []\n",
        "        for j in sample_range:\n",
        "            X_path = os.path.join(self.X_dir, self.X_names[j])\n",
        "            Y_path = os.path.join(self.Y_dir, self.Y_names[j])\n",
        "            \n",
        "            label = [self.PHONEMES.index(yy) for yy in np.load(Y_path)][1:-1]\n",
        "\n",
        "            X_data = np.load(X_path)\n",
        "            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n",
        "            X.append(X_data)\n",
        "            Y.append(np.array(label))\n",
        "        \n",
        "        # keep data separate for each file\n",
        "        X, Y = np.array(X, dtype=object), np.array(Y, dtype=object)\n",
        "        return X, Y\n",
        "\n",
        "# Dataloader\n",
        "class LibriItems(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y, context=0):\n",
        "        '''\n",
        "        Load data by item\n",
        "            X (list of array): batch data of features\n",
        "            Y (list of array): batch data of label\n",
        "            context: specify number of data before and after item\n",
        "        '''\n",
        "\n",
        "        # define data index mapping\n",
        "        index_map_X = []\n",
        "        for i, x in enumerate(X):\n",
        "            for j, xx in enumerate(x):\n",
        "                index_map_X.append((i, j))\n",
        "        \n",
        "        # define label index mapping\n",
        "        index_map_Y = []\n",
        "        for i, y in enumerate(Y):\n",
        "            for j, yy in enumerate(y):\n",
        "                index_map_Y.append((i, j))\n",
        "        \n",
        "        # store variable\n",
        "        assert(set(index_map_X) == set(index_map_Y))\n",
        "        self.length = len(index_map_X)\n",
        "        self.index_map = index_map_X\n",
        "        self.context = context\n",
        "\n",
        "        if context == 0:\n",
        "            # no padding\n",
        "            self.X, self.Y = X, Y\n",
        "        else:\n",
        "            # pad 0 before and after each file\n",
        "            x = []\n",
        "            for i in range(len(X)):\n",
        "                x.append(np.pad(X[i], [(context, context), (0, 0)], mode='constant', constant_values=0))\n",
        "            self.X = np.array(x, dtype=object)\n",
        "            self.Y = Y\n",
        "    \n",
        "    # get the length\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    # get data\n",
        "    def __getitem__(self, index):\n",
        "        # get data by index\n",
        "        i, j = self.index_map[index]\n",
        "        if self.context == 0:\n",
        "            x = self.X[i][j,:].flatten()\n",
        "            y = self.Y[i][j]\n",
        "        else:\n",
        "            x = self.X[i][j:(j + self.context*2 + 1),:].flatten()\n",
        "            y = self.Y[i][j]\n",
        "        return x, y\n",
        "\n",
        "# Dataset for test\n",
        "class LibriTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, partition=\"test-clean\", test_order='test_order.csv', context = 0):\n",
        "        '''\n",
        "        Load data without label\n",
        "            data_path: data path\n",
        "            partition: test directory\n",
        "            test_order: competition file order\n",
        "            context: specify number of data before and after item\n",
        "        '''\n",
        "        self.X_dir = os.path.join(data_path, partition, 'mfcc')\n",
        "        self.X_names = list(pd.read_csv(test_order)['file'])\n",
        "        \n",
        "        # load data for each np file\n",
        "        X = []\n",
        "        for j in range(len(self.X_names)):\n",
        "            X_path = os.path.join(self.X_dir, self.X_names[j])\n",
        "            X_data = np.load(X_path)\n",
        "            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n",
        "            X.append(X_data)\n",
        "        X = np.array(X, dtype=object)\n",
        "\n",
        "        # define data index mapping\n",
        "        index_map_X = []\n",
        "        for i, x in enumerate(X):\n",
        "            for j, xx in enumerate(x):\n",
        "                index_map_X.append((i, j))\n",
        "        self.length = len(index_map_X)\n",
        "        self.index_map = index_map_X\n",
        "        self.context = context\n",
        "\n",
        "        if context == 0:\n",
        "            # no padding\n",
        "            self.X = X\n",
        "        else:\n",
        "            # pad 0 before and after each file\n",
        "            x = []\n",
        "            for i in range(len(X)):\n",
        "                x.append(np.pad(X[i], [(context, context), (0, 0)], mode='constant', constant_values=0))\n",
        "            self.X = np.array(x, dtype=object)\n",
        "\n",
        "    # get length\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get data by index\n",
        "        i, j = self.index_map[index]\n",
        "        if self.context == 0:\n",
        "            x = self.X[i][j,:].flatten()\n",
        "        else:\n",
        "            x = self.X[i][j:(j + self.context*2 + 1),:].flatten()\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "N7y256Vv8Gnj",
      "metadata": {
        "id": "N7y256Vv8Gnj"
      },
      "outputs": [],
      "source": [
        "# Train / Val / Test\n",
        "\n",
        "def train(args, model, device, train_samples, optimizer, criterion, epoch):\n",
        "    '''\n",
        "    Train data from training dataset\n",
        "    '''\n",
        "\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "\n",
        "    # run for each training sample\n",
        "    for i in range(len(train_samples)):\n",
        "        # load dataset and dataloader\n",
        "        X, Y = train_samples[i]\n",
        "        train_items = LibriItems(X, Y, context=args['context'])\n",
        "        train_loader = torch.utils.data.DataLoader(train_items, batch_size=args['batch_size'], \n",
        "                                                   shuffle=True, num_workers=args['num_workers'])\n",
        "        \n",
        "        # run by catch\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # load data to device (cuda)\n",
        "            data = data.float().to(device)\n",
        "            target = target.long().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            # print loss \n",
        "            if batch_idx % args['log_interval'] == 0:\n",
        "                print('Train Epoch: {} \\tBatch: {}/{}[{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, i+1, len(train_samples), \n",
        "                    batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "    \n",
        "    # return average training loss\n",
        "    return np.mean(train_loss)\n",
        "\n",
        "def val(args, model, device, dev_samples, criterion):\n",
        "    '''\n",
        "    Validate data from validated dataset\n",
        "    '''\n",
        "\n",
        "    model.eval()\n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    val_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # loop for each sample\n",
        "        for i in range(len(dev_samples)):\n",
        "            X, Y = dev_samples[i]\n",
        "            # load dataset and dataloader\n",
        "            val_items = LibriItems(X, Y, context=args['context'])\n",
        "            val_loader = torch.utils.data.DataLoader(val_items, batch_size=args['batch_size'], \n",
        "                                                     shuffle=False, num_workers=args['num_workers'])\n",
        "            # predict\n",
        "            for data, true_y in val_loader:\n",
        "                data = data.float().to(device)\n",
        "                true_y = true_y.long().to(device)                \n",
        "                \n",
        "                output = model(data)\n",
        "                loss = criterion(output, true_y)\n",
        "                val_loss.append(loss.item())\n",
        "\n",
        "                pred_y = torch.argmax(output, axis=1)\n",
        "                pred_y_list.extend(pred_y.tolist())\n",
        "                true_y_list.extend(true_y.tolist())\n",
        "\n",
        "    # return average validation loss and accuracy\n",
        "    train_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
        "    return np.mean(val_loss), train_accuracy\n",
        "\n",
        "def test(test_data, model, device, args):\n",
        "    '''\n",
        "    Predict data from test dataset\n",
        "    '''\n",
        "\n",
        "    model.eval()\n",
        "    pred_y_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # load dataloader\n",
        "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=args['batch_size'], \n",
        "                                                  shuffle=False, num_workers=args['num_workers'])\n",
        "\n",
        "        # loop for each item and predict\n",
        "        for data in test_loader:\n",
        "            data = data.float().to(device)             \n",
        "            \n",
        "            output = model(data)\n",
        "            pred_y = torch.argmax(output, axis=1)\n",
        "\n",
        "            pred_y_list.extend(pred_y.tolist())\n",
        "\n",
        "    return pred_y_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "-c1_duaJlBSX",
      "metadata": {
        "id": "-c1_duaJlBSX"
      },
      "outputs": [],
      "source": [
        "# Run model\n",
        "\n",
        "# save model to the path every epoch (last model and best model)\n",
        "def save_model(exp_name, model_path, model, optimizer, epoch, metrics, total_time, best_model=False):\n",
        "    # set experiment path\n",
        "    exp_path = os.path.join(model_path, exp_name)\n",
        "    if os.path.isdir(exp_path) is False:\n",
        "        os.mkdir(exp_path)\n",
        "        print(\"Create dir: {}\".format(exp_path))\n",
        "    # save last model and metrics to run.tar\n",
        "    exp_run_path = os.path.join(exp_path, 'run.tar')\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': metrics[0][-1],\n",
        "        'val_loss': metrics[1][-1],\n",
        "        'val_acc': metrics[2][-1],\n",
        "        'metrics': metrics,\n",
        "        'total_time': total_time\n",
        "        }\n",
        "    torch.save(checkpoint, exp_run_path)\n",
        "    print(\"Save model: {}\".format(exp_run_path))\n",
        "    # if this is the best model save model and metrics to model.tar\n",
        "    if best_model:\n",
        "        exp_best_path = os.path.join(exp_path, 'model.tar')\n",
        "        torch.save(checkpoint, exp_best_path)\n",
        "        print(\"Save model: {}\".format(exp_best_path))\n",
        "\n",
        "# run training\n",
        "def run(model, optimizer, criterion, args, viz=False):\n",
        "    # set experiment name\n",
        "    exp_name = args['exp_name'] + '_' + datetime.datetime.now(pytz.timezone('US/Eastern')).strftime(\"%Y%m%d-%H%M%S\")\n",
        "    # set tensorboard name\n",
        "    if viz:\n",
        "        writer = tensorboard.SummaryWriter(os.path.join(args['log_path'],  exp_name))\n",
        "    \n",
        "    # check device\n",
        "    print('Using device:', next(model.parameters()).device)\n",
        "\n",
        "    # load batch dataset\n",
        "    train_samples = LibriSamples(data_path = args['data_path'], sample=args['sample'], shuffle=True, partition=\"train-clean-100\", csvpath=args.get('csvpath'))\n",
        "    val_samples = LibriSamples(data_path = args['data_path'], shuffle=True, partition=\"dev-clean\")\n",
        "\n",
        "    # loop by epoch\n",
        "    best_loss = 0\n",
        "    trn_loss_list = []\n",
        "    val_loss_list = []\n",
        "    val_acc_list = []\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, args['epoch'] + 1):\n",
        "        # get loss and accuracy value\n",
        "        print('#### EPOCH {} ####'.format(epoch))\n",
        "        trn_loss = train(args, model, device, train_samples, optimizer, criterion, epoch)\n",
        "        val_loss, val_acc = val(args, model, device, val_samples, criterion)\n",
        "\n",
        "        # append data to list\n",
        "        print('** train loss={:.6f} | validation loss={:.6f} | validation accuracy={:.2f}%'.format(\n",
        "            trn_loss, val_loss, val_acc*100.\n",
        "            ))\n",
        "        trn_loss_list.append(trn_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        val_acc_list.append(val_acc)\n",
        "        metrics = np.array([trn_loss_list, val_loss_list, val_acc_list])\n",
        "\n",
        "        # write data to tensor board\n",
        "        if viz:\n",
        "            writer.add_scalar(\"Loss/train\", trn_loss, epoch)\n",
        "            writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
        "            writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
        "\n",
        "        # test best model\n",
        "        if epoch == 1:\n",
        "            best_model = True\n",
        "            best_loss = val_loss\n",
        "        elif val_loss < best_loss:\n",
        "            best_model = True\n",
        "            best_loss = val_loss\n",
        "        else:\n",
        "            best_model = False\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        # save model\n",
        "        save_model(exp_name, args['model_path'], model, optimizer, epoch, metrics, total_time, best_model)\n",
        "    \n",
        "    # close writer\n",
        "    if viz:\n",
        "        writer.flush()\n",
        "\n",
        "# resume training from the last model\n",
        "def resume(model, optimizer, criterion, args, viz=False):\n",
        "    # load model\n",
        "    checkpoint = torch.load(os.path.join(args['model_path'], args['exp_load'], 'run.tar'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    metrics = checkpoint['metrics']\n",
        "\n",
        "    # set experiment name\n",
        "    exp_name = args['exp_name'] + '_' + datetime.datetime.now(pytz.timezone('US/Eastern')).strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    # write existing metrics\n",
        "    if viz:\n",
        "        writer = tensorboard.SummaryWriter(os.path.join(args['log_path'],  exp_name))\n",
        "        for i in range(metrics.shape[1]):\n",
        "            writer.add_scalar(\"Loss/train\", metrics[0][i], i+1)\n",
        "            writer.add_scalar(\"Loss/val\", metrics[1][i], i+1)\n",
        "            writer.add_scalar(\"Accuracy/val\", metrics[2][i], i+1)\n",
        "    \n",
        "    # load batch dataset\n",
        "    train_samples = LibriSamples(data_path = args['data_path'], sample=args['sample'], shuffle=True, partition=\"train-clean-100\", csvpath=args.get('csvpath'))\n",
        "    val_samples = LibriSamples(data_path = args['data_path'], shuffle=True, partition=\"dev-clean\")\n",
        "\n",
        "    # loop by epoch\n",
        "    best_loss = np.min(metrics[1])\n",
        "    trn_loss_list = list(metrics[0])\n",
        "    val_loss_list = list(metrics[1])\n",
        "    val_acc_list = list(metrics[2])\n",
        "    start_time = time.time()\n",
        "    for epoch in range(checkpoint['epoch'] + 1, args['epoch'] + 1):\n",
        "        # get loss and accuracy value\n",
        "        print('#### EPOCH {} ####'.format(epoch))\n",
        "        trn_loss = train(args, model, device, train_samples, optimizer, criterion, epoch)\n",
        "        val_loss, val_acc = val(args, model, device, val_samples, criterion)\n",
        "\n",
        "        # append data to list\n",
        "        print('** train loss={:.6f} | validation loss={:.6f} | validation accuracy={:.2f}%'.format(\n",
        "            trn_loss, val_loss, val_acc*100.\n",
        "            ))\n",
        "        trn_loss_list.append(trn_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        val_acc_list.append(val_acc)\n",
        "        metrics = np.array([trn_loss_list, val_loss_list, val_acc_list])\n",
        "\n",
        "        # write data to tensor board\n",
        "        if viz:\n",
        "            writer.add_scalar(\"Loss/train\", trn_loss, epoch)\n",
        "            writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
        "            writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
        "\n",
        "        # test best model\n",
        "        if epoch == checkpoint['epoch'] + 1:\n",
        "            best_model = True\n",
        "            best_loss = val_loss\n",
        "        elif val_loss < best_loss:\n",
        "            best_model = True\n",
        "            best_loss = val_loss\n",
        "        else:\n",
        "            best_model = False\n",
        "        total_time = time.time() - start_time + checkpoint['total_time']\n",
        "\n",
        "        # save model\n",
        "        save_model(exp_name, args['model_path'], model, optimizer, epoch, metrics, total_time, best_model)\n",
        "    \n",
        "    # close writer\n",
        "    if viz:\n",
        "        writer.flush()\n",
        "\n",
        "# predict model from best model (model.tar)\n",
        "def pred_test(model, device, args):\n",
        "    # load model\n",
        "    checkpoint = torch.load(os.path.join(args['model_path'], args['exp_load'], 'model.tar'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print('predict from model: {} (train loss={:.6f} | validation loss={:.6f} | validation accuracy={:.2f}% | epoch={})'.format(\n",
        "        args['exp_load'], checkpoint['train_loss'], checkpoint['val_loss'], checkpoint['val_acc']*100, checkpoint['epoch']\n",
        "        ))\n",
        "    \n",
        "    # load test dataset\n",
        "    test_data = LibriTest(data_path = args['data_path'], context=args['context'])\n",
        "    y_pred = test(test_data, model, device, args)\n",
        "\n",
        "    # write to csv\n",
        "    df = pd.DataFrame({\n",
        "        \"id\": range(len(y_pred)),\n",
        "        \"label\": y_pred\n",
        "    })\n",
        "    df.to_csv(args['file_out'], index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SZwRXbj9vOwG",
      "metadata": {
        "id": "SZwRXbj9vOwG"
      },
      "source": [
        "# Model Architecture\n",
        "All model architectures that are already trained\n",
        "- The best model is Network08\n",
        "- Exclude 01-04 which haven't saved in this script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "43eQBSlnqoSI",
      "metadata": {
        "id": "43eQBSlnqoSI"
      },
      "outputs": [],
      "source": [
        "class Network05(torch.nn.Module):\n",
        "    '''\n",
        "    Pyramid architecture\n",
        "    Default, no context\n",
        "    '''\n",
        "    def __init__(self, in_size):\n",
        "        super(Network05, self).__init__()\n",
        "        # in_size = 13\n",
        "        layers = [\n",
        "            nn.Linear(in_size, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 40),\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.layers(A0)\n",
        "        return x\n",
        "\n",
        "class Network06(torch.nn.Module):\n",
        "    '''\n",
        "    Pyramid architecture\n",
        "    Increase size of the first layer\n",
        "    '''\n",
        "    def __init__(self, in_size):\n",
        "        super(Network06, self).__init__()\n",
        "        # in_size = 13\n",
        "        layers = [\n",
        "            nn.Linear(in_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 40),\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.layers(A0)\n",
        "        return x\n",
        "\n",
        "class Network07(torch.nn.Module):\n",
        "    '''\n",
        "    Pyramid architecture\n",
        "    Add context\n",
        "    Reduce layers\n",
        "    '''\n",
        "    def __init__(self, context):\n",
        "        super(Network07, self).__init__()\n",
        "        # context = 50\n",
        "        in_size = 13*(args['context']*2+1)\n",
        "        layers = [\n",
        "            nn.Linear(in_size, in_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(in_size),\n",
        "            nn.Linear(in_size, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 40),\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.layers(A0)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Network08(torch.nn.Module):\n",
        "    '''\n",
        "    Cylindrical architecture\n",
        "    Remain context\n",
        "    '''\n",
        "    def __init__(self, context):\n",
        "        super(Network08, self).__init__()\n",
        "        # context = 50\n",
        "        in_size = 13*(args['context']*2+1)\n",
        "        layers = [\n",
        "            nn.Linear(in_size, in_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(in_size),\n",
        "            nn.Linear(in_size, in_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(in_size),\n",
        "            nn.Linear(in_size, in_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(in_size),\n",
        "            nn.Linear(in_size, in_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(in_size),\n",
        "            nn.Linear(in_size, in_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(in_size),\n",
        "            nn.Linear(in_size, in_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(in_size),\n",
        "            nn.Linear(in_size, in_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(in_size),\n",
        "            nn.Linear(in_size, in_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(in_size),\n",
        "            nn.Linear(in_size, 40),\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.layers(A0)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6uJfGfRovX9I",
      "metadata": {
        "id": "6uJfGfRovX9I"
      },
      "source": [
        "# Run / Resume"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nFkqFzYVg603",
      "metadata": {
        "id": "nFkqFzYVg603"
      },
      "source": [
        "## Run\n",
        "Train new experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EzPmidNCc_Mz",
      "metadata": {
        "id": "EzPmidNCc_Mz"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Argument variables\n",
        "    Batch size: batch size to load for each batch\n",
        "    context: relevant context around x to load for prediction\n",
        "    log_interval: interval for printing each log\n",
        "    data_path: path to get data\n",
        "    lr: learning rate\n",
        "    epoch: how many epochs to run for this experiment\n",
        "    exp_name: experiment name to save\n",
        "    sample: how many files to load for each loop\n",
        "    num_workers: number of workers to run\n",
        "    model_path: path of the model\n",
        "    log_path: path of the log file\n",
        "'''\n",
        "\n",
        "args = {\n",
        "    'batch_size': 2048,\n",
        "    'context': 50,\n",
        "    'log_interval': 1000,\n",
        "    'data_path': '/content/hw1p2_student_data',\n",
        "    'lr': 0.001,\n",
        "    'epoch': 100,\n",
        "    'exp_name': 'layer_08',\n",
        "    'sample': 10000,\n",
        "    'num_workers': 2,\n",
        "    'model_path': MODEL_PATH,\n",
        "    'log_path': LOG_PATH,\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Set model architecture\n",
        "model = Network08(args['context']).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "run(model, optimizer, criterion, args, viz=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y5SAUEIohKZv",
      "metadata": {
        "id": "y5SAUEIohKZv"
      },
      "source": [
        "## Resume\n",
        "Resume existing model to continue training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0agdG4Apc4e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0agdG4Apc4e0",
        "outputId": "24d65b7e-44bf-4cc7-ce5c-15e42b711df9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EPOCH 82 ####\n",
            "Train Epoch: 82 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.362907\n",
            "Train Epoch: 82 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.371115\n",
            "Train Epoch: 82 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.367860\n",
            "Train Epoch: 82 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.372688\n",
            "Train Epoch: 82 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.389365\n",
            "Train Epoch: 82 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.400370\n",
            "Train Epoch: 82 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.354776\n",
            "Train Epoch: 82 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.418745\n",
            "Train Epoch: 82 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.378728\n",
            "Train Epoch: 82 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.376037\n",
            "Train Epoch: 82 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.425448\n",
            "Train Epoch: 82 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.418343\n",
            "Train Epoch: 82 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.397052\n",
            "Train Epoch: 82 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.421043\n",
            "Train Epoch: 82 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.443758\n",
            "Train Epoch: 82 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.406905\n",
            "Train Epoch: 82 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.392570\n",
            "Train Epoch: 82 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.396836\n",
            "Train Epoch: 82 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.400332\n",
            "Train Epoch: 82 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.399273\n",
            "** train loss=0.386572 | validation loss=0.392432 | validation accuracy=87.00%\n",
            "Create dir: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/model.tar\n",
            "#### EPOCH 83 ####\n",
            "Train Epoch: 83 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.400889\n",
            "Train Epoch: 83 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.407309\n",
            "Train Epoch: 83 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.361413\n",
            "Train Epoch: 83 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.426873\n",
            "Train Epoch: 83 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.390322\n",
            "Train Epoch: 83 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.394211\n",
            "Train Epoch: 83 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.390445\n",
            "Train Epoch: 83 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.378708\n",
            "Train Epoch: 83 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.389828\n",
            "Train Epoch: 83 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.335249\n",
            "Train Epoch: 83 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.398465\n",
            "Train Epoch: 83 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.395015\n",
            "Train Epoch: 83 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.395650\n",
            "Train Epoch: 83 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.394849\n",
            "Train Epoch: 83 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.370111\n",
            "Train Epoch: 83 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.403867\n",
            "Train Epoch: 83 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.352129\n",
            "Train Epoch: 83 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.378358\n",
            "Train Epoch: 83 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.358022\n",
            "Train Epoch: 83 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.394662\n",
            "** train loss=0.386364 | validation loss=0.393749 | validation accuracy=87.00%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 84 ####\n",
            "Train Epoch: 84 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.385140\n",
            "Train Epoch: 84 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.395111\n",
            "Train Epoch: 84 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.389409\n",
            "Train Epoch: 84 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.376854\n",
            "Train Epoch: 84 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.365293\n",
            "Train Epoch: 84 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.372454\n",
            "Train Epoch: 84 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.395516\n",
            "Train Epoch: 84 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.373407\n",
            "Train Epoch: 84 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.383200\n",
            "Train Epoch: 84 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.403770\n",
            "Train Epoch: 84 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.396402\n",
            "Train Epoch: 84 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.421697\n",
            "Train Epoch: 84 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.416676\n",
            "Train Epoch: 84 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.376320\n",
            "Train Epoch: 84 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.383047\n",
            "Train Epoch: 84 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.374736\n",
            "Train Epoch: 84 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.437638\n",
            "Train Epoch: 84 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.390439\n",
            "Train Epoch: 84 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.385488\n",
            "Train Epoch: 84 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.369185\n",
            "** train loss=0.386098 | validation loss=0.393132 | validation accuracy=87.00%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 85 ####\n",
            "Train Epoch: 85 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.393679\n",
            "Train Epoch: 85 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.388930\n",
            "Train Epoch: 85 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.394032\n",
            "Train Epoch: 85 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.370011\n",
            "Train Epoch: 85 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.385002\n",
            "Train Epoch: 85 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.377437\n",
            "Train Epoch: 85 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.388556\n",
            "Train Epoch: 85 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.405407\n",
            "Train Epoch: 85 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.383628\n",
            "Train Epoch: 85 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.426547\n",
            "Train Epoch: 85 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.350790\n",
            "Train Epoch: 85 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.377861\n",
            "Train Epoch: 85 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.362821\n",
            "Train Epoch: 85 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.408769\n",
            "Train Epoch: 85 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.369171\n",
            "Train Epoch: 85 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.358694\n",
            "Train Epoch: 85 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.400956\n",
            "Train Epoch: 85 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.376631\n",
            "Train Epoch: 85 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.389608\n",
            "Train Epoch: 85 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.369779\n",
            "** train loss=0.385867 | validation loss=0.394563 | validation accuracy=86.98%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 86 ####\n",
            "Train Epoch: 86 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.369945\n",
            "Train Epoch: 86 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.377886\n",
            "Train Epoch: 86 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.347175\n",
            "Train Epoch: 86 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.349758\n",
            "Train Epoch: 86 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.428674\n",
            "Train Epoch: 86 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.385933\n",
            "Train Epoch: 86 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.371788\n",
            "Train Epoch: 86 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.356770\n",
            "Train Epoch: 86 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.377246\n",
            "Train Epoch: 86 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.404568\n",
            "Train Epoch: 86 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.408209\n",
            "Train Epoch: 86 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.399374\n",
            "Train Epoch: 86 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.376565\n",
            "Train Epoch: 86 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.423009\n",
            "Train Epoch: 86 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.383775\n",
            "Train Epoch: 86 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.397488\n",
            "Train Epoch: 86 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.390564\n",
            "Train Epoch: 86 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.368109\n",
            "Train Epoch: 86 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.365683\n",
            "Train Epoch: 86 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.407818\n",
            "** train loss=0.385389 | validation loss=0.393495 | validation accuracy=86.99%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 87 ####\n",
            "Train Epoch: 87 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.399213\n",
            "Train Epoch: 87 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.406124\n",
            "Train Epoch: 87 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.373653\n",
            "Train Epoch: 87 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.411546\n",
            "Train Epoch: 87 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.365552\n",
            "Train Epoch: 87 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.406890\n",
            "Train Epoch: 87 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.390049\n",
            "Train Epoch: 87 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.378229\n",
            "Train Epoch: 87 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.386073\n",
            "Train Epoch: 87 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.414990\n",
            "Train Epoch: 87 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.403066\n",
            "Train Epoch: 87 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.379452\n",
            "Train Epoch: 87 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.408473\n",
            "Train Epoch: 87 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.410936\n",
            "Train Epoch: 87 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.379751\n",
            "Train Epoch: 87 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.366314\n",
            "Train Epoch: 87 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.395140\n",
            "Train Epoch: 87 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.393978\n",
            "Train Epoch: 87 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.400709\n",
            "Train Epoch: 87 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.415701\n",
            "** train loss=0.385274 | validation loss=0.392652 | validation accuracy=87.02%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 88 ####\n",
            "Train Epoch: 88 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.388616\n",
            "Train Epoch: 88 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.395879\n",
            "Train Epoch: 88 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.361354\n",
            "Train Epoch: 88 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.387332\n",
            "Train Epoch: 88 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.396592\n",
            "Train Epoch: 88 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.358893\n",
            "Train Epoch: 88 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.401375\n",
            "Train Epoch: 88 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.408068\n",
            "Train Epoch: 88 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.402664\n",
            "Train Epoch: 88 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.366457\n",
            "Train Epoch: 88 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.373805\n",
            "Train Epoch: 88 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.401147\n",
            "Train Epoch: 88 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.372671\n",
            "Train Epoch: 88 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.379228\n",
            "Train Epoch: 88 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.395667\n",
            "Train Epoch: 88 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.410836\n",
            "Train Epoch: 88 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.363389\n",
            "Train Epoch: 88 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.367494\n",
            "Train Epoch: 88 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.414791\n",
            "Train Epoch: 88 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.397395\n",
            "** train loss=0.384703 | validation loss=0.393611 | validation accuracy=86.99%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 89 ####\n",
            "Train Epoch: 89 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.384896\n",
            "Train Epoch: 89 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.394531\n",
            "Train Epoch: 89 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.361645\n",
            "Train Epoch: 89 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.379303\n",
            "Train Epoch: 89 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.408914\n",
            "Train Epoch: 89 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.371891\n",
            "Train Epoch: 89 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.380307\n",
            "Train Epoch: 89 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.407965\n",
            "Train Epoch: 89 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.388747\n",
            "Train Epoch: 89 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.410953\n",
            "Train Epoch: 89 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.384259\n",
            "Train Epoch: 89 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.407431\n",
            "Train Epoch: 89 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.369897\n",
            "Train Epoch: 89 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.356444\n",
            "Train Epoch: 89 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.410049\n",
            "Train Epoch: 89 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.396230\n",
            "Train Epoch: 89 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.398513\n",
            "Train Epoch: 89 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.394315\n",
            "Train Epoch: 89 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.373568\n",
            "Train Epoch: 89 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.367245\n",
            "** train loss=0.384481 | validation loss=0.393717 | validation accuracy=87.01%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 90 ####\n",
            "Train Epoch: 90 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.408578\n",
            "Train Epoch: 90 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.423173\n",
            "Train Epoch: 90 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.373030\n",
            "Train Epoch: 90 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.387002\n",
            "Train Epoch: 90 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.400597\n",
            "Train Epoch: 90 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.383457\n",
            "Train Epoch: 90 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.406615\n",
            "Train Epoch: 90 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.367189\n",
            "Train Epoch: 90 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.378914\n",
            "Train Epoch: 90 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.372531\n",
            "Train Epoch: 90 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.384691\n",
            "Train Epoch: 90 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.396599\n",
            "Train Epoch: 90 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.380364\n",
            "Train Epoch: 90 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.376125\n",
            "Train Epoch: 90 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.373225\n",
            "Train Epoch: 90 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.384121\n",
            "Train Epoch: 90 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.393137\n",
            "Train Epoch: 90 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.371071\n",
            "Train Epoch: 90 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.400507\n",
            "Train Epoch: 90 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.387368\n",
            "** train loss=0.384135 | validation loss=0.392638 | validation accuracy=87.05%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 91 ####\n",
            "Train Epoch: 91 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.394766\n",
            "Train Epoch: 91 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.364747\n",
            "Train Epoch: 91 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.374358\n",
            "Train Epoch: 91 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.371950\n",
            "Train Epoch: 91 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.407461\n",
            "Train Epoch: 91 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.369055\n",
            "Train Epoch: 91 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.364674\n",
            "Train Epoch: 91 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.366141\n",
            "Train Epoch: 91 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.391910\n",
            "Train Epoch: 91 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.408040\n",
            "Train Epoch: 91 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.376096\n",
            "Train Epoch: 91 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.402999\n",
            "Train Epoch: 91 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.376647\n",
            "Train Epoch: 91 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.365171\n",
            "Train Epoch: 91 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.398210\n",
            "Train Epoch: 91 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.390368\n",
            "Train Epoch: 91 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.403158\n",
            "Train Epoch: 91 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.403262\n",
            "Train Epoch: 91 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.391920\n",
            "Train Epoch: 91 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.350634\n",
            "** train loss=0.383924 | validation loss=0.393196 | validation accuracy=87.03%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 92 ####\n",
            "Train Epoch: 92 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.389697\n",
            "Train Epoch: 92 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.390872\n",
            "Train Epoch: 92 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.380350\n",
            "Train Epoch: 92 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.369753\n",
            "Train Epoch: 92 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.398517\n",
            "Train Epoch: 92 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.369115\n",
            "Train Epoch: 92 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.406477\n",
            "Train Epoch: 92 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.388325\n",
            "Train Epoch: 92 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.383675\n",
            "Train Epoch: 92 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.379314\n",
            "Train Epoch: 92 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.375946\n",
            "Train Epoch: 92 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.407997\n",
            "Train Epoch: 92 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.401876\n",
            "Train Epoch: 92 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.376797\n",
            "Train Epoch: 92 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.374908\n",
            "Train Epoch: 92 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.367394\n",
            "Train Epoch: 92 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.396462\n",
            "Train Epoch: 92 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.442217\n",
            "Train Epoch: 92 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.392241\n",
            "Train Epoch: 92 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.375183\n",
            "** train loss=0.383651 | validation loss=0.393616 | validation accuracy=87.02%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 93 ####\n",
            "Train Epoch: 93 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.389070\n",
            "Train Epoch: 93 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.391448\n",
            "Train Epoch: 93 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.415096\n",
            "Train Epoch: 93 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.415301\n",
            "Train Epoch: 93 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.376251\n",
            "Train Epoch: 93 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.395537\n",
            "Train Epoch: 93 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.404481\n",
            "Train Epoch: 93 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.378993\n",
            "Train Epoch: 93 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.395395\n",
            "Train Epoch: 93 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.368027\n",
            "Train Epoch: 93 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.347898\n",
            "Train Epoch: 93 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.391773\n",
            "Train Epoch: 93 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.369269\n",
            "Train Epoch: 93 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.367309\n",
            "Train Epoch: 93 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.405650\n",
            "Train Epoch: 93 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.381508\n",
            "Train Epoch: 93 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.398837\n",
            "Train Epoch: 93 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.415901\n",
            "Train Epoch: 93 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.377551\n",
            "Train Epoch: 93 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.364535\n",
            "** train loss=0.383315 | validation loss=0.393566 | validation accuracy=87.01%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 94 ####\n",
            "Train Epoch: 94 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.391795\n",
            "Train Epoch: 94 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.382539\n",
            "Train Epoch: 94 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.431157\n",
            "Train Epoch: 94 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.336208\n",
            "Train Epoch: 94 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.384422\n",
            "Train Epoch: 94 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.354371\n",
            "Train Epoch: 94 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.389246\n",
            "Train Epoch: 94 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.394137\n",
            "Train Epoch: 94 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.413778\n",
            "Train Epoch: 94 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.401714\n",
            "Train Epoch: 94 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.363342\n",
            "Train Epoch: 94 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.369700\n",
            "Train Epoch: 94 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.401730\n",
            "Train Epoch: 94 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.369165\n",
            "Train Epoch: 94 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.420048\n",
            "Train Epoch: 94 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.411883\n",
            "Train Epoch: 94 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.370037\n",
            "Train Epoch: 94 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.381746\n",
            "Train Epoch: 94 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.402678\n",
            "Train Epoch: 94 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.349566\n",
            "** train loss=0.382970 | validation loss=0.391769 | validation accuracy=87.07%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/model.tar\n",
            "#### EPOCH 95 ####\n",
            "Train Epoch: 95 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.400126\n",
            "Train Epoch: 95 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.385697\n",
            "Train Epoch: 95 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.359900\n",
            "Train Epoch: 95 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.380560\n",
            "Train Epoch: 95 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.379232\n",
            "Train Epoch: 95 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.386050\n",
            "Train Epoch: 95 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.382538\n",
            "Train Epoch: 95 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.370947\n",
            "Train Epoch: 95 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.415214\n",
            "Train Epoch: 95 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.393473\n",
            "Train Epoch: 95 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.378791\n",
            "Train Epoch: 95 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.389496\n",
            "Train Epoch: 95 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.385222\n",
            "Train Epoch: 95 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.384716\n",
            "Train Epoch: 95 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.363578\n",
            "Train Epoch: 95 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.375563\n",
            "Train Epoch: 95 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.376207\n",
            "Train Epoch: 95 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.381400\n",
            "Train Epoch: 95 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.386806\n",
            "Train Epoch: 95 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.381050\n",
            "** train loss=0.382723 | validation loss=0.393196 | validation accuracy=87.02%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 96 ####\n",
            "Train Epoch: 96 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.376501\n",
            "Train Epoch: 96 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.381459\n",
            "Train Epoch: 96 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.361925\n",
            "Train Epoch: 96 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.387594\n",
            "Train Epoch: 96 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.374712\n",
            "Train Epoch: 96 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.369279\n",
            "Train Epoch: 96 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.354041\n",
            "Train Epoch: 96 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.393547\n",
            "Train Epoch: 96 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.346758\n",
            "Train Epoch: 96 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.371212\n",
            "Train Epoch: 96 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.404523\n",
            "Train Epoch: 96 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.382084\n",
            "Train Epoch: 96 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.394429\n",
            "Train Epoch: 96 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.390233\n",
            "Train Epoch: 96 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.378546\n",
            "Train Epoch: 96 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.408739\n",
            "Train Epoch: 96 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.392464\n",
            "Train Epoch: 96 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.376757\n",
            "Train Epoch: 96 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.379384\n",
            "Train Epoch: 96 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.414113\n",
            "** train loss=0.382484 | validation loss=0.393753 | validation accuracy=87.04%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 97 ####\n",
            "Train Epoch: 97 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.425144\n",
            "Train Epoch: 97 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.388849\n",
            "Train Epoch: 97 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.390423\n",
            "Train Epoch: 97 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.346513\n",
            "Train Epoch: 97 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.403280\n",
            "Train Epoch: 97 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.395454\n",
            "Train Epoch: 97 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.364435\n",
            "Train Epoch: 97 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.384801\n",
            "Train Epoch: 97 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.371997\n",
            "Train Epoch: 97 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.400130\n",
            "Train Epoch: 97 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.393079\n",
            "Train Epoch: 97 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.373399\n",
            "Train Epoch: 97 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.401934\n",
            "Train Epoch: 97 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.372382\n",
            "Train Epoch: 97 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.410088\n",
            "Train Epoch: 97 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.386513\n",
            "Train Epoch: 97 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.386716\n",
            "Train Epoch: 97 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.365874\n",
            "Train Epoch: 97 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.376109\n",
            "Train Epoch: 97 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.367893\n",
            "** train loss=0.382113 | validation loss=0.392054 | validation accuracy=87.05%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 98 ####\n",
            "Train Epoch: 98 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.406173\n",
            "Train Epoch: 98 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.388653\n",
            "Train Epoch: 98 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.394890\n",
            "Train Epoch: 98 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.406982\n",
            "Train Epoch: 98 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.389157\n",
            "Train Epoch: 98 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.361541\n",
            "Train Epoch: 98 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.358985\n",
            "Train Epoch: 98 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.393530\n",
            "Train Epoch: 98 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.347496\n",
            "Train Epoch: 98 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.363611\n",
            "Train Epoch: 98 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.364616\n",
            "Train Epoch: 98 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.394258\n",
            "Train Epoch: 98 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.363639\n",
            "Train Epoch: 98 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.386716\n",
            "Train Epoch: 98 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.395957\n",
            "Train Epoch: 98 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.411649\n",
            "Train Epoch: 98 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.418969\n",
            "Train Epoch: 98 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.369677\n",
            "Train Epoch: 98 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.351924\n",
            "Train Epoch: 98 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.373087\n",
            "** train loss=0.381687 | validation loss=0.394056 | validation accuracy=87.03%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "#### EPOCH 99 ####\n",
            "Train Epoch: 99 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.361138\n",
            "Train Epoch: 99 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.357917\n",
            "Train Epoch: 99 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.394320\n",
            "Train Epoch: 99 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.351169\n",
            "Train Epoch: 99 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.368711\n",
            "Train Epoch: 99 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.373547\n",
            "Train Epoch: 99 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.364329\n",
            "Train Epoch: 99 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.365588\n",
            "Train Epoch: 99 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.364452\n",
            "Train Epoch: 99 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.376815\n",
            "Train Epoch: 99 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.356065\n",
            "Train Epoch: 99 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.351395\n",
            "Train Epoch: 99 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.368351\n",
            "Train Epoch: 99 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.334641\n",
            "Train Epoch: 99 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.378146\n",
            "Train Epoch: 99 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.364051\n",
            "Train Epoch: 99 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.368644\n",
            "Train Epoch: 99 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.361491\n",
            "Train Epoch: 99 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.397016\n",
            "Train Epoch: 99 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.402211\n",
            "** train loss=0.381520 | validation loss=0.391669 | validation accuracy=87.09%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/model.tar\n",
            "#### EPOCH 100 ####\n",
            "Train Epoch: 100 \tBatch: 1/3[0/12677455 (0%)]\tLoss: 0.416467\n",
            "Train Epoch: 100 \tBatch: 1/3[2048000/12677455 (16%)]\tLoss: 0.417070\n",
            "Train Epoch: 100 \tBatch: 1/3[4096000/12677455 (32%)]\tLoss: 0.363456\n",
            "Train Epoch: 100 \tBatch: 1/3[6144000/12677455 (48%)]\tLoss: 0.409761\n",
            "Train Epoch: 100 \tBatch: 1/3[8192000/12677455 (65%)]\tLoss: 0.387833\n",
            "Train Epoch: 100 \tBatch: 1/3[10240000/12677455 (81%)]\tLoss: 0.363383\n",
            "Train Epoch: 100 \tBatch: 1/3[12288000/12677455 (97%)]\tLoss: 0.381499\n",
            "Train Epoch: 100 \tBatch: 2/3[0/12663079 (0%)]\tLoss: 0.392491\n",
            "Train Epoch: 100 \tBatch: 2/3[2048000/12663079 (16%)]\tLoss: 0.386155\n",
            "Train Epoch: 100 \tBatch: 2/3[4096000/12663079 (32%)]\tLoss: 0.352730\n",
            "Train Epoch: 100 \tBatch: 2/3[6144000/12663079 (49%)]\tLoss: 0.388386\n",
            "Train Epoch: 100 \tBatch: 2/3[8192000/12663079 (65%)]\tLoss: 0.360725\n",
            "Train Epoch: 100 \tBatch: 2/3[10240000/12663079 (81%)]\tLoss: 0.373921\n",
            "Train Epoch: 100 \tBatch: 2/3[12288000/12663079 (97%)]\tLoss: 0.361305\n",
            "Train Epoch: 100 \tBatch: 3/3[0/10850600 (0%)]\tLoss: 0.341412\n",
            "Train Epoch: 100 \tBatch: 3/3[2048000/10850600 (19%)]\tLoss: 0.372336\n",
            "Train Epoch: 100 \tBatch: 3/3[4096000/10850600 (38%)]\tLoss: 0.379984\n",
            "Train Epoch: 100 \tBatch: 3/3[6144000/10850600 (57%)]\tLoss: 0.390533\n",
            "Train Epoch: 100 \tBatch: 3/3[8192000/10850600 (75%)]\tLoss: 0.375557\n",
            "Train Epoch: 100 \tBatch: 3/3[10240000/10850600 (94%)]\tLoss: 0.362598\n",
            "** train loss=0.381192 | validation loss=0.392110 | validation accuracy=87.04%\n",
            "Save model: /content/drive/MyDrive/CMU/dev/dl-hw1/model/layer_08_20220208-091953/run.tar\n"
          ]
        }
      ],
      "source": [
        "# Resume\n",
        "'''\n",
        "Argument variables\n",
        "    Batch size: batch size to load for each batch\n",
        "    context: relevant context around x to load for prediction\n",
        "    log_interval: interval for printing each log\n",
        "    data_path: path to get data\n",
        "    lr: learning rate\n",
        "    epoch: how many epochs to run for this experiment\n",
        "    exp_name: experiment name to save\n",
        "    exp_load: experiment name to load when resuming model\n",
        "    sample: how many files to load for each loop\n",
        "    num_workers: number of workers to run\n",
        "    model_path: path of the model\n",
        "    log_path: path of the log file\n",
        "'''\n",
        "\n",
        "args = {\n",
        "    'batch_size': 2048,\n",
        "    'context': 50,\n",
        "    'log_interval': 1000,\n",
        "    'data_path': '/content/hw1p2_student_data',\n",
        "    'lr': 0.001,\n",
        "    'epoch': 100,\n",
        "    'exp_name': 'layer_08',\n",
        "    'exp_load': 'layer_08_20220208-012704',\n",
        "    'sample': 10000,\n",
        "    'num_workers': 2,\n",
        "    'model_path': MODEL_PATH,\n",
        "    'log_path': LOG_PATH,\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Network08(args['context']).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#run(model, optimizer, criterion, args, viz=True)\n",
        "resume(model, optimizer, criterion, args, viz=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "roS6B2p0vbdR",
      "metadata": {
        "id": "roS6B2p0vbdR"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Re-FhVmyha0E",
      "metadata": {
        "id": "Re-FhVmyha0E"
      },
      "source": [
        "## Predict\n",
        "Predict test dataset to output file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "trMobfDbxhpO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trMobfDbxhpO",
        "outputId": "8e0630f9-c7ec-486e-cde4-86e7967477c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict from model: layer_08_20220208-091953 (train loss=0.381520 | validation loss=0.391669 | validation accuracy=87.09% | epoch=99)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Argument variables\n",
        "    Batch size: batch size to load for each batch\n",
        "    context: relevant context around x to load for prediction\n",
        "    data_path: path to get data\n",
        "    exp_load: experiment name to load when resuming model\n",
        "    sample: how many files to load for each loop\n",
        "    num_workers: number of workers to run\n",
        "    model_path: path of the model\n",
        "    file_out: output file name\n",
        "'''\n",
        "\n",
        "args = {\n",
        "    'batch_size': 2048,\n",
        "    'context': 50,\n",
        "    'data_path': '/content/hw1p2_student_data',\n",
        "    'exp_load': 'layer_08_20220208-091953',\n",
        "    'sample': 10000,\n",
        "    'num_workers': 2,\n",
        "    'model_path': MODEL_PATH,\n",
        "    'file_out': 'submission_10.csv'\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Network08(args['context']).to(device)\n",
        "pred_test(model, device, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZnjetNq9hjT_",
      "metadata": {
        "id": "ZnjetNq9hjT_"
      },
      "source": [
        "## Upload\n",
        "Upload output file to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "umzbwxgXxBIB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umzbwxgXxBIB",
        "outputId": "d0d41496-cb4b-4158-ace3-bbcb8beb6020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "100% 18.6M/18.6M [00:02<00:00, 6.57MB/s]\n",
            "Successfully submitted to Frame-Level Speech Recognition"
          ]
        }
      ],
      "source": [
        "submission_file = args['file_out']\n",
        "! kaggle competitions submit -c 11-785-s22-hw1p2 -f $submission_file -m \"Submission 10\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "z2tmvmkSWAzG",
        "eU2be0pcX9AW",
        "3ggXQgug2CLS",
        "ANlV727juvP0",
        "SZwRXbj9vOwG",
        "6uJfGfRovX9I",
        "roS6B2p0vbdR"
      ],
      "machine_shape": "hm",
      "name": "hw1p2_s22_02_submit.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('11785')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f6e51205b7216008b187321cf9e304e6214d1044d3b41f96cd917a91dd56122f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
